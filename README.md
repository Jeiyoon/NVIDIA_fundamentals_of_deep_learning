## Topics

| RMSE/MSE |  Adam/Adagrad/RMSProp  |
| :---: | :---: |
| Softmax classifiers |  SGD/GD  |
| Cross-entropy | CNNs |
| RNNs/LSTMs | GANs |
| AutoEncoders | Reinforcement Learning |
| Natural Language Processing (basics) | Training, validation, and test datasets |
| Pooling | Padding |
| Overfitting | Hyperparameters |
| Features | Parameters |
| Activation functions | Transfer Learning |
| Frameworks | Image Classification |
| Dropout | Forwards and backwards propagation |

## Optimizers

- “An Overview of Gradient Descent Optimization Algorithms”: https://arxiv.org/pdf/1609.04747.pdf

- SGD optimizer (Josh Starmer): https://www.youtube.com/watch?v=vMh0zPT0tLI (Note: A little campy)

● Gradient Descent with Momentum (Andrew Ng):
https://www.youtube.com/watch?v=k8fTYJPd3_I
○ Note: A little extra information

● RSMProp optimizer (Andrew Ng): https://www.youtube.com/watch?v=_e-LFe_igno

● Adam optimizer (Andrew Ng): https://www.youtube.com/watch?v=JXQT_vxqwIs
○ Warning: Scary math ahead

● Adagrad optimizer (minutes 5:29 - 9:19):
https://www.youtube.com/watch?v=gmwxUy7NYpA
